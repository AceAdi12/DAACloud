# â˜ï¸ Cloud File Storage Optimization System using Huffman Coding

This project is a cloud-based file storage solution that uses **Huffman compression**, **SHA-256 hashing**, and **AWS S3 integration** to optimize storage and retrieval of files. It prevents duplicate uploads and speeds up file access using caching and MongoDB-based metadata tracking.

---

## ğŸ§  Key Features

- âœ… Compress files using **Huffman coding** (greedy algorithm)
- âœ… Calculate **SHA-256 hash** to detect and skip duplicate files
- âœ… Upload compressed files to **AWS S3**
- âœ… Store file metadata (name, original name, hash) in **MongoDB**
- âœ… Retrieve and **decompress files** with cache-first approach
- âœ… Visualize **compression efficiency** using graphs
- âœ… Fully interactive **Tkinter GUI**

---

## ğŸ“¦ Technology Stack

| Category         | Tools / Tech                |
|------------------|-----------------------------|
| Programming      | Python                      |
| Cloud Storage    | AWS S3                      |
| Hashing          | SHA-256 (`hashlib`)         |
| Compression      | Huffman Coding              |
| Database         | MongoDB                     |
| GUI              | Tkinter                     |
| Visualization    | Matplotlib                  |
| File Operations  | Pickle, OS, File I/O        |

---

## ğŸ“Š Project Progress

- âœ… File upload with hashing
- âœ… Huffman compression implemented
- âœ… Duplicate check using MongoDB
- âœ… AWS S3 integration complete
- âœ… GUI fully functional
- âœ… File retrieval + decompression done
- âœ… Graph plotting added

---

## ğŸ§ª Testing Summary

| Test Type              | Status     | Notes                                   |
|------------------------|------------|-----------------------------------------|
| Upload (new file)      | âœ… Pass     | Compressed and uploaded to S3           |
| Upload (duplicate)     | âœ… Pass     | Detected and skipped                    |
| Decompression          | âœ… Pass     | Restores file from `.huff` format       |
| Cache check            | âœ… Pass     | Fast retrieval from local cache         |
| Graph plotting         | âœ… Pass     | Shows original vs compressed size       |
| GUI interaction        | âœ… Pass     | All buttons and features responsive     |

---

## ğŸ“ Folder Structure
ğŸ“¦ DAACloudProject/
â”œâ”€â”€ hashing_utils.py
â”œâ”€â”€ huffman_utils.py
â”œâ”€â”€ decompress.py
â”œâ”€â”€ s3_upload_with_hash.py
â”œâ”€â”€ file_retrieval.py
â”œâ”€â”€ main_app.py
â”œâ”€â”€ config.py
â”œâ”€â”€ README.md
â””â”€â”€ cache/ # Local cache folder


---

## ğŸ“Œ How to Run

1. Clone the repo
2. Install dependencies (boto3, pymongo, matplotlib)
3. Ensure MongoDB is running
4. Set your AWS keys in `config.py`
5. Run:  
   ```bash
   python gui_app.py

---
Sample Output
File1.txt â†’ File1_compressed.huff

Original size: 48 KB â†’ Compressed: 16 KB

Retrieval time: 1.02 seconds

Duplicate upload: Skipped

